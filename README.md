# Grapes Detection using VGG-19 Convolutional Neural Network and Gram Matrices
I used the VGG-19 Convolutional Neural Network to detect images containg grapes (in all sort of shapes and colors). 
## Description  
This is my first **Machine Learning** project: it was a lot of fun and i learned a lot about **CNNs** and its usage in image detection/classification
The idea was to view the problem of **grapes** detection (or even the more general image detection) into a binary classification problem and use the **VGG-19** as a simple style extractor to obtain the '**style**' of grapes to then use as input of classification layer which give, as final output, the **probability** of an image containing grapes. This Approach is often described as [transfer learning](https://cs231n.github.io/transfer-learning/) and in particular in my case I'm using the '**ConvNet as a fixed feature extractor**' method of transfer learning, where the base CNN is used with ***pretrained weights*** (and all its parameters frozen) and I'm only training the classification layer (in my case I've chosen a simple logistic regression but a two layer classification neural net would work fine as well.
Note: the fully connected classification layers that VGG19 comes with are removed and replaced with logistic regression.
## Model Inputs
Model inputs always have to be **4D array** consisting of (batch_size,height,weight,depth) like it's shown in the image below:

![VGG19_input](https://user-images.githubusercontent.com/83078138/222975487-b9b99032-7681-4f64-8b4b-9ccb55fff636.jpg)

In my case the input images are RGB images of size (224,224,3) with a batches of 50 images
For my model I have used in total **4000** images (both images contaning grapes and not) splitted into **75%** training samples, and **12,5%** for testing and validation samples(each) like shown in the picture below:

![meta-chart](https://user-images.githubusercontent.com/83078138/222976931-e517e9b2-2be8-421b-aeb4-132e2b7efe3a.png)

## Model Architecture
VGG-19 contain a combination of layers which transform an image into output that the model can understand.

![vgg19-1024x173](https://user-images.githubusercontent.com/83078138/222979190-3bb1a4d2-9cf8-4b68-bf79-e5092a60d78a.png)

***-Convolutional layer***: creates a feature map by applying a filter that scans the image several pixels at a time

***-Pooling layer***: scales down the information generated by the convolutional layer to effectively store it

To extract the **style** of an image, and not its deteiled features, we need to apply the [**Gram Matrix**](https://en.wikipedia.org/wiki/Gram_matrix) to the output of some of the vgg's layers. But why we use the Gran matrix for style extraction? 
Consider two vectors(more specifically 2 **flattened** feature vectors from a convolutional feature map of depth C) representing features of the input space, and their dot product give us the information about the relation between them. The lesser the product the more different the learned features are and greater the product, the more correlated the features are. In other words, the lesser the product, the lesser the two features co-occur and the greater it is, the more they occur together. This in a sense gives information about an ***imageâ€™s style(texture)*** and zero information about its spatial structure, since we already flatten the feature and perform dot product on top of it. Here's a rapresentation of the gram matrix computation: 


![The-process-of-Gram-matrix-computation](https://user-images.githubusercontent.com/83078138/226183334-332209a6-932d-44b2-bddc-73f047c94e3c.png)


In my case I've applied the gram matrix to the first convolutional layer of the first, second, third and fifth convolutional blocks (layers 'conv1_1', 'conv2_1', 'conv3_1', 'conv5_1'). I then trained 4 different models where the style extraction occurs a the 4 different layer and compared the results (see below).

The final classification layer consists of a simple [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) using the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function)(see image below) to calculate the probability that the given image (or more precisely, its style features) contains grapes:

![logistic curve](https://user-images.githubusercontent.com/83078138/224567842-729afab6-71ec-454e-a001-4a39c2b163a8.png)

## Code
Raw images are first preprocessed in 'image_preprocessing.py' to assure they are all **RGB** format (3 channels) and **splitted** (via the [Split-Folders library](https://pypi.org/project/split-folders/)) into the folder structure shown below:

![folder structure](https://user-images.githubusercontent.com/83078138/222977832-9fc3f9e0-a5f2-4cb6-9377-04a484178999.PNG)
         
where Class0 folders contains images without grapes, while Class1 contains images with grapes.
Here's the code for image processing: 
```python
def Convert_to_RGB(path):
    for i,image in enumerate(os.listdir(path)):
        im = Image.open(path+image)
        if im.mode != 'RGB':
            print("image converted: ",image)
            im.convert("RGB").save(path + f"RGB_image{i}.jpg")
            os.remove(path+image)

if __name__ =='__main__':
    #Convert all images into RGB form
    Convert_to_RGB(raw_data_path + 'Class0//')
    Convert_to_RGB(raw_data_path + 'Class1//')
    #perform train,validation,test split
    splitfolders.fixed(input=raw_data_path,output=processed_data_path,fixed=(1500,250,250))
```

The file GrapesDetector.py contains the model definition,initialization,training,validation,testing and results visualization.
Firstly we initialize our **hyperparameters**:
```python
#hyperparameters
train_dir='data//processed_data//train' 
val_dir='data//processed_data//val'
test_dir='data//processed_data//test'
epochs=25
learning_rate=0.001
image_size=224
batch_size=25
n_features=[102400,409600,1638400,6553600] #feature for each style layers
mean=[0.485, 0.456, 0.406]
std=[0.229, 0.224, 0.225]
style_layers = ['conv1_1','conv2_1','conv3_1','conv5_1']
device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
#defining loss function (Binary cross Entropy Loss) Note: loss_fn is the same in all cases (vgg/resnet/train/test) 
loss_fn=nn.BCELoss()

train_loader = transforms.Compose([
    transforms.Resize((image_size,image_size)),  # scale imported image
    transforms.RandomHorizontalFlip(0.5), #data augmentation
    transforms.RandomVerticalFlip(0.5),
    transforms.RandomRotation(np.random.randint(0,360)),
    transforms.ToTensor(),
    transforms.Normalize(mean,std)
    ])  # transform it into a torch tensor

test_loader = transforms.Compose([
    transforms.Resize((image_size,image_size)),  # scale imported image
    transforms.ToTensor(),
    transforms.Normalize(mean,std)
    ])  # transform it into a torch tensor
```
where ***n_feature*** is the list of style features extracted with the gram matrix for each style layer.

Note that the training data is randomly flipped and rotated just the help avoid overfitting (with a bit of **data augmentation**)
and all images are first normalized before passing it through the CNN.

Firsly we define our style extraction model based on VGG19:
```python
#Defining a class for the style features extractor model
class VGG(nn.Module):
    def __init__(self,layer):
        super(VGG,self).__init__()
        self.style_layers=['0','5','10','28']
        self.layer=self.style_layers[layer]
        #Since we need only the some layers in the model we will be dropping all the rest layers from the features of the model
        self.model=models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:(int(self.layer)+1)] #model will contain only the necessary layers
        #freezing vgg weights
        for param in self.model.parameters():
            param.requires_grad = False

    #x holds the input tensor(image) that will be feeded to each layer
    def forward(self,x):
        #Iterate over all the layers of the mode
        for layer_num,layer in enumerate(self.model):
            #activation of the layer will stored in x
            x=layer(x)
            #appending the activation of the selected layers and return the gram matrix
            if str(layer_num) == self.layer:
                return gram_matrix(x)
```
Note that only part of the vgg features layers are used, depending on which layer we've chosen for style extraction. The input is forwarded to each layer of the vgg model until we reach the style extraction layer and the model return the gram matrix of the output of that specific layer.
The gram matrix is defined as the following:
```python
#gram_matrix function
def gram_matrix(input):
    b,c,h,w=input.shape
    # b=batch size
    # c=number of feature maps
    # (h,w)=dimensions of a f. map (N=c*d)
    features=input.view(b*c,h*w)
    G = torch.mm(features, features.t())  # compute the gram product
    # we 'normalize' the values of the gram matrix
    # by dividing by the number of element in each feature maps.
    return G.div(b*c*h*w)
```
Note that the output of this function will have (bxc,bxc) shape, where b is the batch size and c is the number of feature maps.
Then we define our classification model:
```python
#Grapes detection Neural network
class GrapesDetector(nn.Module):
    def __init__(self,n_feature):
        super(GrapesDetector, self).__init__()
        self.linear = nn.Linear(n_feature,1)
        self.in_features=n_feature
        
    def forward(self, x):
        x = x.view(-1, self.in_features)
        y = torch.sigmoid(self.linear(x))
        return y
```
Here we have simple **Neural Network** with only one layer which takes as input the style features extracted by the CNN and generate, through the **sigmoid function** (see above), and returns
a probability(between 0 and 1) that the input features contains grapes.
After that we have the classic training and testing function:
```python
#training function
def train_model(train_loader,val_loader,style_extractor,model,loss_fn,optimizer,batch_size,epochs):

    #initialize list for later plotting results
    history=[]
    train_losses=[]
    train_accs=[]
    val_losses=[]
    val_accs=[]
    train_samples=len(train_loader.dataset)
    train_batches=len(train_loader)
    validation_samples=len(val_loader.dataset)
    validation_baches=len(val_loader)

    #training loop 
    for epoch in range(epochs):

        #training phase
        model.train() 
        print(f"Epoch {epoch+1}")
        print("training phase...")
        t_loss=0
        t_correct=0
        for inputs,label in train_loader:
            #forward pass
            inputs,label=inputs.to(device,torch.float),label.to(device,torch.float)
            style_features=style_extractor(inputs)
            pred=model(style_features).view(batch_size)
            
            loss=loss_fn(pred,label)
            t_loss+=loss.item()
            
            #Calculating training accuracy
            pred=pred.detach().round()
            for t in range(batch_size):
                if (pred[t]==label[t]).item():
                    t_correct += 1
            
            #apply backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        t_loss /= train_batches
        t_correct /= train_samples
        train_losses.append(t_loss)
        train_accs.append(t_correct*100)
        print("training loss: ",t_loss)
        print("training accuracy: ",t_correct*100)

        print("Training Done!")
        
        #validation phase
        model.eval()
        print("validation phase...")
        
        with torch.no_grad():
            v_loss=0
            v_correct=0
            for inputs, labels in val_loader:

                inputs=inputs.to(device,torch.float)
                labels=labels.to(device,torch.float)
                style_features=style_extractor(inputs)
                pred = model(style_features).view(batch_size)

                loss=loss_fn(pred, labels)
                v_loss += loss.item()

                #Calculating validation accuracy
                pred=pred.detach().round()
                for t in range(batch_size):
                    if (pred[t]==labels[t]).item():
                        v_correct +=1

            v_loss /= validation_baches
            v_correct /= validation_samples
            val_losses.append(v_loss)
            val_accs.append(v_correct*100) 
            print("validation loss: ",v_loss)
            print("validation accuracy: ",v_correct*100)

        print("Validation Done!")
        print("-------------------------------")
    
    history.append(train_losses)
    history.append(train_accs)
    history.append(val_losses)
    history.append(val_accs)
    return history
```
Here we can see the classic training (and validation) pipeline:
Firstly we use the built-in pytorch dataloaders to iterate through the input images in batches.
The model trains throughout many **epochs** (25 in our case are more than sufficient) by taking one **forward** and one **backward** pass of all training samples each time. Forward propagation calculates the loss and cost functions by comparing the difference between the actual and predicted target for each labeled image.
Backward propagation uses [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20a%20replacement%20optimization,sparse%20gradients%20on%20noisy%20problems.) optimizer to update the weights and bias for each neuron, attributing more impact on the neurons which have the most predictive power, until it arrives to an optimal activation combination (**global minimum**)(see image below).

![gradient descent](https://user-images.githubusercontent.com/83078138/222984187-2655f905-ce66-4a89-a3a5-5cb3f4b41b58.jpg)

As the model sees more examples, it learns to better predict the target causing the loss measure to decrease
The cost function takes the average loss across all samples indicating overall performance. 
In my case the **loss function** is, of course, the ***Binary Cross Entropy Loss***, which is defined as the negative average of the log of corrected predicted probabilities. It's standard for binary classification problem like mine:

![BCE Loss](https://user-images.githubusercontent.com/83078138/222997616-3ac39fad-1b6b-4500-86a7-72be863436f5.jpg)

Where **N** is the batch size, **pi** is the probability of class 1,**(1-pi)** is the probability of class 0 , while **yi** is the actual class of the given image.
When the prediction belongs to class 1 the first part of the formula becomes active and the second part vanishes and vice versa.

After every training epoch the model is tested with the validation dataset 
to check model performance and improvements through the epochs (this process is called **validation**).
Finally,  after the training phase has finished we plot our results (in particular loss and accuracy history of our model).  

Then we have the testing function, where we test our already trained model with the test dataset:
```python
#testing function        
def test_model(test_loader,style_extractor,model,loss_fn,batch_size):
    model.eval()
    test_samples = len(test_loader.dataset)
    test_batches = len(test_loader)
    t_loss=0
    t_correct=0
    random_number=np.random.randint(0,len(test_loader))

    with torch.no_grad():

        for i,(inputs, labels) in enumerate(test_loader):

            inputs=inputs.to(device,torch.float)
            labels=labels.to(device,torch.float)
            style_features=style_extractor(inputs)
            pred = model(style_features).view(batch_size)
            loss=loss_fn(pred, labels)
            loss=loss.item()
            t_loss += loss

            #select random batch to visualize predictions
            #if(i==random_number):
                #show_random_prediction(inputs,pred,num=6)
            
            #checking testing accuracy
            pred=pred.detach().round()
            for t in range(batch_size):
                if (pred[t]==labels[t]).item():
                    t_correct +=1

        t_loss /= test_batches
        t_correct /= test_samples
        t_loss=np.round(t_loss,decimals=3)
        t_correct=np.round(t_correct*100,decimals=2)

    return t_loss,t_correct
```
The remaining function are all helper function for plotting and visualizing results and images
## Results
I compared the 4 models in terms of loss, accuracy and time (both in training phase and in the testing phase).
### Training

After 25 epochs of training the models history looks like this:

![train_loss](https://user-images.githubusercontent.com/83078138/226459448-32595b79-528d-4abd-b06a-e4ebfe12428b.png)
![train_acc](https://user-images.githubusercontent.com/83078138/226459485-dd1626ee-61bb-45ce-8f62-d9eb53a72387.png)
![val_loss](https://user-images.githubusercontent.com/83078138/226459514-279629ab-0dc4-47a7-b717-52a7e08a24ef.png)
![val_acc](https://user-images.githubusercontent.com/83078138/226459553-fdaf1b4a-5e18-4f8d-bba7-9065df184775.png)
![training_time](https://user-images.githubusercontent.com/83078138/226459653-db9b3647-3597-4973-8960-7b893bf17cec.png)

### Testing

In my testing the models on the test dataset (500 images) the results are this: 

![test_losses](https://user-images.githubusercontent.com/83078138/226183473-962075f8-612d-470a-b666-1f2b43ff5702.png)
![test_accs](https://user-images.githubusercontent.com/83078138/226183476-6694d664-c26e-4cdb-98e8-5e71023c0045.png)
![test_times](https://user-images.githubusercontent.com/83078138/226183479-3d4ddc55-ed90-48d2-b2fa-a8a7a9936a30.png)

Here's an example of the model prediction with 6 random images from the test dataset:

![test_batch_example](https://user-images.githubusercontent.com/83078138/222992540-94f81def-6de9-486a-8c9b-d59b41c9632f.png)


We can see that, the more we go deep into the vgg19 CNN, the more the losses decreaces and the accuracy increaces, reaching around **97%** if we use the ***conv5_1*** layer for style extraction. From that we can derive that the more we get away from the input image, the more we lose 
spatial information and detailed features (a.k.a. the **content** of the image), and we better extract information about image style and texture
(what we are looking for).
One negative aspect, as it's clearly shown in the results, is that the deeper you go, the more time is required for the forwanding through the vgg model and ,most importantly, the computation of the gram matrix (the number of features increases rapidly as we into the deeper layers) becomes very slow.

## Usage
To use this project you need to have **python 3** installed on your system and install the following python **libraries**:

1)torch

2)torchvision

3)PIL

4)os

5)argparse

6)matplolib.pyplot

7)numpy

8)splitfolders

9)time

This can be done by simply executing in your teminal the command:
pip install [library_name]
for every library listed above.
Finally, if you want to try training and testing, on your own images, you can (you will need at least 2000 images for each class). Just create (in the data folder of the project) two subfolders: **raw_data**, where you'll need to put your own images divided into two subfolders(one for each class), and **processed_data**, which will contain the splitted datasets (just leave it empty for now). Then you need (only for the first time) to run 'py image_preprocessing.py command to check that all the images are in the right format and to split them into the different datasets(filling the processed_data folder). After this you are good to go:
just run 'GrapesDetector.py command. 

**Note**: to run GrapesDetector you'll need to pass an argument:  
run ***py GrapesDetectorStyle.py train*** if you want to train your own models, ***py GrapesDetectorStyle.py test*** if you want to test the saved models. At the end of training the models will be saved in the 'models' folder of the project as 'grepes_detector_vgg{i}.pt' (where i=0,1,2,3) respectively. The project comes in with the models which i have already trained and tested with 4000 samples which you can test straight away. Have fun with it! :)





